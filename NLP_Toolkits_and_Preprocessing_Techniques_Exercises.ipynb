{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Toolkits and Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [review data from Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews) to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie. \n",
    "\n",
    "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Downloads/NLP_v01-class2/data/cookie_reviews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Determine how many reviews there are in total.\n",
    "* Determine the percent of 1, 2, 3, 4 and 5 star reviews.\n",
    "* Determine the distribution of character lengths for the reviews, by listing the values and by plotting a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Question 1##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   1.                          Determine how many reviews there are in total.\n",
    "num_of_reviews = len(data)\n",
    "num_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   2.                         Determine the percent of 1, 2, 3, 4 and 5 star reviews.\n",
    "one_star = []\n",
    "two_star = []\n",
    "Three_star = []\n",
    "four_star = []\n",
    "Five_star = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    #print(row[1])\n",
    "    if row[1] == 1:\n",
    "        one_star.append(row[1])\n",
    "    if row[1] == 2:\n",
    "        two_star.append(row[1])\n",
    "    if row[1] == 3:\n",
    "        Three_star.append(row[1])\n",
    "    if row[1] == 4:\n",
    "        four_star.append(row[1])\n",
    "    if row[1] == 5:\n",
    "        Five_star.append(row[1])\n",
    "\n",
    "print(\"Rating Percentages: \")\n",
    "print()\n",
    "\n",
    "###        percent of 1 star \n",
    "num1 = len(one_star)\n",
    "num2 = len(data)\n",
    "result = '{0:.2f}%'.format((num1 / num2 * 100))\n",
    "print('1_star : ',result)\n",
    "\n",
    "\n",
    "###     percent of 2 star \n",
    "num1 = len(two_star)\n",
    "num2 = len(data)\n",
    "\n",
    "\n",
    "result = '{0:.2f}%'.format((num1 / num2 * 100))\n",
    "print('2_star : ',result)\n",
    "\n",
    "###     percent of 3 star \n",
    "num1 = len(Three_star)\n",
    "num2 = len(data)\n",
    "result = '{0:.2f}%'.format((num1 / num2 * 100))\n",
    "print('3_star : ', result)\n",
    "\n",
    "###     percent of 4 star \n",
    "num1 = len(four_star)\n",
    "num2 = len(data)\n",
    "result = '{0:.2f}%'.format((num1 / num2 * 100))\n",
    "print('4_star :',result)\n",
    "\n",
    "###     percent of 5 star \n",
    "num1 = len(Five_star)\n",
    "num2 = len(data)\n",
    "result = '{0:.2f}%'.format((num1 / num2 * 100))\n",
    "print('5_star ',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  3   Determine the distribution of character lengths for the reviews, by listing the values and by plotting a histogram.\n",
    "\n",
    "#data['reviews'].value_counts()\n",
    "#data['reviews'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Apply the following preprocessing steps:\n",
    "\n",
    "     1. Remove all words that contain numbers\n",
    "     2. Make all the text lowercase\n",
    "     3. Remove punctuation\n",
    "     4. Tokenize the reviews into words\n",
    "     \n",
    "  Hint #1: Use regular expressions.\n",
    "  \n",
    "  Hint #2: The cookie review in the second row has numbers, upper case letters and punctuation. You can use it to test out your regular expressions.\n",
    "     \n",
    "     \n",
    "* Find the most common words.\n",
    "* Determine the word length distribution over the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                              Apply the following preprocessing steps:\n",
    "\n",
    "\n",
    "###   1. Remove all words that contain numbers\n",
    "dd = data['reviews']\n",
    "\n",
    "new_list_removed_number = []\n",
    "for i in range(len(dd)):\n",
    "    texts = dd[i]\n",
    "    new_text = re.sub('\\w*\\d\\w*', ' ', texts)\n",
    "    new_list_removed_number.append(new_text)\n",
    "\n",
    "print(new_list_removed_number)\n",
    "print(len(new_list_removed_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   2. Make all the text lowercase\n",
    "\n",
    "new_list_lower_case = []\n",
    "for i in range(len(new_list_removed_number)):\n",
    "    texts = new_list_removed_number[i].lower()\n",
    "    new_list_lower_case.append(texts)\n",
    "\n",
    "print(new_list_lower_case)\n",
    "print(len(new_list_lower_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   3. Remove punctuation\n",
    "\n",
    "new_list_removed_punct = []\n",
    "for i in range(len(new_list_lower_case)):\n",
    "    texts = new_list_lower_case[i]\n",
    "    new_txt = re.sub('[%s]' % re.escape(string.punctuation), ' ', texts)\n",
    "    new_list_removed_punct.append(new_txt)\n",
    "\n",
    "print(new_list_removed_punct)\n",
    "print(len(new_list_removed_punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Tokenize the reviews into words\n",
    "\n",
    "new_list_word_tokenize = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "for i in range(len(new_list_removed_punct)):\n",
    "    texts = new_list_removed_punct[i]\n",
    "    new_txts = word_tokenize(texts)\n",
    "    new_list_word_tokenize.append([new_txts])\n",
    "\n",
    "print(new_list_word_tokenize)\n",
    "print(len(new_list_word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  #                             Find the most common words.\n",
    "\n",
    "import collections\n",
    "\n",
    "most_common_words = []\n",
    "counted = []\n",
    "\n",
    "for i in range(len(new_list_word_tokenize)):\n",
    "    texts = new_list_word_tokenize[i]\n",
    "    texts = sum(texts, [])\n",
    "    counter = collections.Counter(texts)\n",
    "    #print(counter.most_common())\n",
    "    counted.append(counter.most_common())\n",
    "\n",
    "all_nums = []\n",
    "for i in range(len(counted)):\n",
    "    texts = counted[i]\n",
    "    for ii in texts:\n",
    "        #print(ii[1])\n",
    "        all_nums.append(ii[1])\n",
    "\n",
    "max_num = max(all_nums) # get the maximum of the counts\n",
    "#print(max_num)\n",
    "\n",
    "for i in range(len(counted)):\n",
    "    texts = counted[i]\n",
    "    for ii in texts:\n",
    "        if ii[1] > max_num - 15  :\n",
    "            if ii[0] not in most_common_words:\n",
    "                most_common_words.append(ii[0])\n",
    "            \n",
    "print('most common words  = ', most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply the following preprocessing techniques:\n",
    "\n",
    "     * Remove stopwords\n",
    "     * Perform parts of speech tagging\n",
    "     * Perform stemming\n",
    "     * Optional: Perform lemmatization\n",
    "\n",
    "  Recommendation: Create a new column in your data set for every preprocessing technique you apply, so you can see the progression of the reviews text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 Apply the following preprocessing techniques:\n",
    "\n",
    "### Remove stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('br') # lets add the 'br' to stopwords as it's the most common and doesn't make sense either.\n",
    "Stop_W = []\n",
    "\n",
    "new_list_remove_stopwords = []\n",
    "for i in range(len(new_list_word_tokenize)):\n",
    "    texts = new_list_word_tokenize[i]\n",
    "    texts = sum(texts, [])\n",
    "    new_tt = [word for word in texts if word not in stop_words]\n",
    "    new_t = [word for word in texts if word in stop_words] # words that are removed \n",
    "    new_list_remove_stopwords.append(new_tt)\n",
    "    Stop_W.append(new_t)\n",
    "    \n",
    "#print('Stop_Words   = ', Stop_W)\n",
    "print(new_list_remove_stopwords)\n",
    "print(len(new_list_remove_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Perform parts of speech tagging\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "new_list_Parts_of_speech_tagging = []\n",
    "for i in range(len(new_list_remove_stopwords)):\n",
    "    texts = new_list_remove_stopwords[i]\n",
    "    tokens = pos_tag(texts)\n",
    "    new_list_Parts_of_speech_tagging.append(tokens)\n",
    "\n",
    "\n",
    "    \n",
    "#print('Stop_Words   = ', Stop_W)\n",
    "print(new_list_Parts_of_speech_tagging)\n",
    "print(len(new_list_Parts_of_speech_tagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###      Perform stemming\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\n",
    "new_list_Perform_stemming = []\n",
    "for i in range(len(new_list_remove_stopwords)):\n",
    "    texts = new_list_remove_stopwords[i]\n",
    "    mew = []\n",
    "    for ii in texts:\n",
    "        #texts = sum(ii, [])\n",
    "        texts = stemmer.stem(ii)\n",
    "        mew.append(texts)\n",
    "    new_list_Perform_stemming.append(mew)\n",
    "    \n",
    "print(new_list_Perform_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new Dataframe(adding the processed 'reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# initialise data of lists. \n",
    "\n",
    "userID = []\n",
    "rating = []\n",
    "list_reviews = []\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    userID.append(row[0])\n",
    "    rating.append(row[1])\n",
    "    list_reviews.append(row[2])\n",
    "\n",
    "dataa = {'user_id':userID, 'stars':rating, 'reviews': list_reviews, 'processed_reviews':new_list_Perform_stemming} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df = pd.DataFrame(dataa) \n",
    "  \n",
    "# print the data \n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 4 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After going through these preprocessing steps, what are the most common words now? Do they make more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                    most common words now\n",
    "\n",
    "new_list_Perform_stemming = new_list_Perform_stemming\n",
    "\n",
    "\n",
    "import collections\n",
    "\n",
    "most_common_words = []\n",
    "counted = []\n",
    "\n",
    "for i in range(len(new_list_Perform_stemming)):\n",
    "    texts = new_list_Perform_stemming[i]\n",
    "    counter = collections.Counter(texts)\n",
    "    counted.append(counter.most_common())\n",
    "\n",
    "all_nums = []\n",
    "for i in range(len(counted)):\n",
    "    texts = counted[i]\n",
    "    for ii in texts:\n",
    "        all_nums.append(ii[1])\n",
    "\n",
    "max_num = max(all_nums) # get the maximum of the counts\n",
    "\n",
    "for i in range(len(counted)):\n",
    "    texts = counted[i]\n",
    "    for ii in texts:\n",
    "        if ii[1] == max_num - 7  :\n",
    "            if ii[0] not in most_common_words:\n",
    "                most_common_words.append(ii[0])\n",
    "            \n",
    "print('most common words  = ', most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Do the common words make more sense?\n",
    "\n",
    "print('Yeah :)' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
